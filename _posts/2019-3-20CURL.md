---
layout: post
title: Contrastive Unsupervised Learning of Semantic Representations: A Theory Framework
date:  2019-3-19 23:00:00
author: Sanjeev Arora and Nikunj Saunshi
visible: False
---

*Semantic representations* (aka  *semantic embeddings*) of complicated data types (e.g. images, text, video) have become central in machine learning, and also crop up in machine translation, language models, GANs, domain transfer, etc. 
These involve learning a *representation function* $f$ such that for any data point $x$ its representation $f(x)$ is  "high level" (retains semantic information while discarding low level details, such as color of individual pixels in an image) and "compact" (low dimensional). The test of a good representation is that it should greatly simplify solving *new* classification tasks, by allowing them to be solved via linear classifiers (or other low-complexity classifiers) using small amounts of labeled data. 


Researchers are most interested in *unsupervised* representation learning using unlabeled data. A popular approach is to use 
objectives similar to the **word2vec** algorithm for word embeddings, and it works well for diverse data types such as molecules, social networks, images, text etc. See the [wikipage of word2vec](https://en.wikipedia.org/wiki/Word2vec) for references. 
Why do such objectives succeed in such diverse settings? This post is about an explanation for these methods by using our [new theoretical framework](https://arxiv.org/abs/1902.09229) with coauthors Hrishikesh Khandeparker, Misha Khodak, and Orestis Plevrakis. As a bonus this framework also yields principled ways to design new variants of the training objectives. The framework uses minimalistic assumptions, which is a good thing, since word2vec-like algorithms apply to vastly different data types and it is unlikely that they can share a common bayesian generative model for the data. (An example of generative models in this space is described in this earlier [blog post on the RAND-WALK model](http://www.offconvex.org/2016/02/14/word-embeddings-2/).


## Semantic representations learning 
Do good, broadly useful representations even *exist* in the first place? In domains such as images, we know the answer is "yes" because deep convolutional neural networks (CNNs) trained to high accuracy on large multiclass labeled datasets such as [ImageNet](http://www.image-net.org/) end up learning very powerful and succinct representations along the way. 
In fact, the penultimate layer of the net --- the input into the final [softmax layer](https://en.wikipedia.org/wiki/Softmax_function) --- serves as a good semantic embedding of the image in new unrelated visual tasks. (Other layers from the trained net can also serve as good embeddings.) In fact availability of such embeddings have led to a revolution in computer vision, allowing a host of new classification tasks to be solved with low-complexity classifiers (eg linear classifiers) using very little labeled data. Thus they are the gold standard to compare to if we try to learn embeddings via unlabeled data. 

<p style="text-align:center;">
<img src="/assets/CURLheadless1.svg" width="45%" />
</p>



### word2vec-like methods: CURL

Since the success of [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), similar approaches were used to learn embeddings for [sentences and paragraphs](https://arxiv.org/pdf/1803.02893.pdf), [image](https://arxiv.org/abs/1505.00687) and [biological sequences](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716/).
These methods share a key idea: they leverage access to pairs of similar data points $x, x^+$, and learn an embedding function $f$ such that the inner product of $f(x)$ and $f(x^+)$ is on average higher than the inner product of $f(x)$ and $f(x^-)$, where $x^{-}$ is a random data point (and thus presumably dissimilar to $x$). 
In practice similar data points are often found heuristically, often using co-occurences, e.g. consecutive sentences in a large text corpus, nearby frames in a video clip, different patches in the same image, etc.

A good example of such methods is **Quick Thoughts** (QT) from [Logeswaran and Lee 2018](https://arxiv.org/pdf/1803.02893.pdf) which is the state-of-the-art unsupervised text embedding. To learn a representation function $f$, QT minimizes the following loss function on a large text corpus
$$L_{un}(f):=\mathop{\mathbb{E}}\limits_{x,x^+,x^-}\left[-\log\left(\frac{e^{f(x)^Tf(x^+)}}{e^{f(x)^Tf(x^+)}+e^{f(x)^Tf(x^-)}}\right)\right]\ \ \ \ \ (1)$$
where $(x, x^+)$ are consecutive sentences and presumable "semantically similar" and $x^-$ is a random negative sample.
(For images $x$ and $x^+$ could be [nearby frames](https://arxiv.org/abs/1505.00687) from a video)
For example, the following are two successive sentences in the wikipedia page on word2vec: *"High frequency words often provide little information."* and *"Words with frequency above a certain threshold may be subsampled to increase training speed."*
Clearly they are much more similar than a random pair of sentences, and the learner exploits this. 
From now on we use *Contrastive Unsupervised Representation learning (CURL)* to refer to  methods that leverage similar pairs of data points and our goal is to analyze these methods.


### Why a new framework for the analysis?

The standard framework for machine learning involves minimizing some loss function, and learning is said to succeed (or *generalize*) if the loss is roughly the same on the average training data point and the average test data point.
In contrastive learning, however, the objective used at test time is very different from the training objective: generalization error is not the right way to think about this. 

> **Main Hurdle for Theory:** We have to show that doing well on task A (minimizing the word2vec-like objective) allows the representation to do well on task B (i.e., classification tasks revealed later). 

Earlier methods along such lines include *kernel learning* and *semi-supervised learning* but there training typically require at least a few labeled examples from the classification tasks that will be seen in future. Bayesian approaches using generative models are also well-established in simpler settings, but have proved difficult for complicated data such as images and text. Furthermore, the simple word2vec-like learners described above do not appear to operate like bayesian optimizers in any obvious way, and furthermore, work for very different data types. 

## Our framework: main elements

Clearly, the implicit/heuristic notion of similarity used in contrastive learning is connected with the downstream tasks in some way --- e.g., similarity carries a strong *hint* that on average the "similar pairs" tend to be assigned the same labels in many downstream tasks (though there is no hard guarantee per se). We present a simple and minimalistic framework to formalize such a notion of similarity. 
For purposes of exposition we'll refer to data points as "images". 

We assume nature has many *classes* of images, and has a  measure $\rho$ on a set of classes $\mathcal{C}$, so that if asked to pick a class  it selects $c$  with probability $\rho(c)$. Each class $c$ also has an associated distribution $D_c$ on images i.e. if nature is asked to furnish examples of class $c$ (e.g., the class "dogs") then it picks image $x$ with probability $D_c(x)$. Note that classes can have arbitrary overlap, including no overlap. 
To formalize a notion of *semantic similarity* we assume that when asked to provide "similar" images, nature picks a class $c^+$ from $\mathcal{C}$ using measure $\rho$ and then picks two i.i.d. samples $x, x^{+}$ from the distribution $D_{c^+}$. The dissimilar example $x^{-}$ is picked by selecting another class $c^-$ from measure $\rho$ and picking a random sample $x^{-}$ from $D_{c^-}$.

<p style="text-align:center;">
<img src="/assets/CURLframework.svg" width="45%" />
</p>

The training objective for learning the representation is exactly the QT objective from earlier, but now inherits the following interpretation from the framework
$$\min_{f\in\mathcal{F}}\ L_{un}(f)=\mathop{\mathbb{E}}\limits_{c^+,c^-\sim\rho}\ \mathop{\mathbb{E}}\limits_{x,x^+\sim D_{c^+}}\ \mathop{\mathbb{E}}\limits_{x^-\sim D_{c^-}}\left[\log\left(1+e^{f(x)^Tf(x^-)-f(x)^Tf(x^+)}\right)\right]$$

Note that the function class $\mathcal{F}$ is an arbitrary deep net architecture mapping images to embeddings, and one would learn $f$ via gradient descent/back-propagation as usual. 
Of course, no theory currently exists for explaining when optimization succeeds for complicated deep nets, so our framework will simply assume that gradient descent has already resulted in some representation $f$ that achieves low loss, and studies how well this does in downstream classification tasks.


### Testing the representation

What defines a good representation? We assume that the quality of the representation is tested by using it to solve a binary (i.e., two-way) classification task using a linear classifier. (The paper also studies extensions to $k$-way classification in the downstream task.) How is this binary classification task selected? Nature picks two classes $c_1, c_2$ randomly according to measure $\rho$ and picks data points for each class according to the associated probability distributions $D_{c_1}$ and $D_{c_2}$. The representation is used to solve this binary task via logistic regression: namely, find two vectors $w_1, w_2$ so as to minimize the following loss
$$L_{sup}(f, (c_1,c_2))=\inf\limits_{w_1,w_2}\left[\frac{1}{2}\mathop{\mathbb{E}}\limits_{x\sim D_{c_1}}\log(1+e^{f(x)^Tw_2-f(x)^Tw_1})+\frac{1}{2}\mathop{\mathbb{E}}\limits_{x\sim D_{c_2}}\log(1+e^{f(x)^Tw_1-f(x)^Tw_2})\right]$$

The quality of the representation is estimated as the *expected* loss over nature's choices of binary classification tasks.
$$L_{sup}(f)=\mathop{\mathbb{E}}\limits_{c_1,c_2\sim\rho}\ \left[L_{sup}(f, (c_1,c_2))|c_1\neq c_2\right]$$

It is important to note that the latent classes present in the unlabeled data are the *same* classes present in the classification tasks. 
This allows us to formalize a sense of 'semantic similarity' as alluded to above: the classes from which data points appear together more frequently are the classes that make up *relevant* classification tasks. 
Note that if the number of classes is large, then typically the data used in unsupervised training may involve *no samples* from the classes used at test time. Indeed, we are hoping to show that the learnt representations are useful in classification on potentially unseen classes.

## Connecting unsupervised and supervised learning

What would be a dream result for theory? Suppose we fix a class of representation functions ${\mathcal F}$ --- say, those computable by a ResNet 50 architecture with some choices of layer sizes etc. 

> **Dream Theorem:** Minimizing the unsupervised loss (using modest amount of unlabeled data) yields a representation function $f \in {\mathcal F}$ that is competitive with the **best** representation from ${\mathcal F}$ on downstream classification tasks, even with very few labeled data per task.

While the number of unlabeled data pairs needed to learn an approximate minimizer can be controlled using Rademacher complexity arguments (see paper), we show that the dream theorem is impossible as phrased: we can exhibit simple class ${\mathcal F}$ where the contrastive objective does not yield representations even remotely competitive with the best in the class.
This should not be surprising and it only suggests that further progress would require making more assumptions than the above minimalistic ones.
Our paper makes progress by showing that under the above framework, if the unsupervised loss happens to be small at the end of contrastive learning then the resulting representations perform well on downstream classification. 

> **Simple Lemma:** The average classification loss on downstream binary tasks is upper bounded by the unsupervised loss.
$$L_{sup}(f)\le \alpha L_{un}(f),~\forall f\in\mathcal{F}\ \ \ \ \ \ (2)$$
where $\alpha$ depends on $\rho$. ($\alpha\rightarrow 1$ when $\mathcal{C}\rightarrow\infty$, for uniform $\rho$) 

This says that the unsupervised loss function can be treated as a **surrogate** for the performance on downstream supervised tasks solved using linear classification, thus minimizing it makes sense.
Furthermore, only few labeled samples are needed just to learn the linear classifiers in future downstream tasks.
Thus our minimalistic framework lets us show guarantees for contrastive learning and also highlights the labeled sample complexity benefits provided by it.

## Extensions of the theoretical analysis

This conceptual framework not only allows us to reason about empirically successful variants of (1), but also leads to the design of new, theoretically grounded unsupervised objective functions. Here we give a high level view of these results and invite you to read our [paper](https://arxiv.org/abs/1902.09229) for more details.

A priori one might imagine that the log and exponentials in (1) have some information theoretic interpretation; here we relate the functional form to the fact that logistic regression is going to be used in the downstream classification tasks. Analogously, if the classification is done via hinge loss, then (2) is true for a different unsupervised loss that uses a hinge-like loss instead. This objective, for instance, was used to learn image representations from videos by [Wang and Gupta](https://arxiv.org/abs/1505.00687). Also, usually in practice $k>1$ negative samples are contrasted with each positive sample $(x,x^+)$ and the unsupervised objective looks like the $k$-class cross-entropy loss. We prove a statement similar to (2) for this setting, where the the supervised loss now is the average $(k+1)$-way classification loss.

Finally, the framework provides guidelines for designing an unsupervised objective when *blocks* of similar data are available (e.g., sentences in a paragraph). Replacing $f(x^+)$ and $f(x^-)$ in (1) with the average of the representations from the positive and the negative block respectively, we get a new objective which comes with stronger guarantees and better performance in practice.


## Experiments

To verify components of the theory, we performed controlled experiments by simulating the data generation process of the framework. Due to lack of a canonical multiclass problem for text, we constructed a 3029-way supervised task by using articles from Wikipedia, where the article topics are classes and sentences within the article are data points. Similar data points are just pairs of sentences sampled from the same article. We learn representations using a simple LSTM architecture by minimizing the above unsupervised loss. The table below shows that the average $k$-way supervised performance of the representations learnt using contrastive learning is very close to that of representations learnt using supervised learning. Additionally, even though not covered by our theory, it also performs respectably on the all-way multiclass problem.

<p style="text-align:center;">
<img src="/assets/CURLexperiment.svg" width="60%" />
</p>

## Conclusions
While contrastive learning is a well-known *intuitive* algorithm, its practical success has been a mystery for theory. Our conceptual framework lets us formally show guarantees for representations learnt using such algorithms. While shedding light on such algorithms, the framework also lets us come up with and analyze variants of it. It also provides insights into what guarantees are provable and shapes the search for new assumptions that would allow stronger guarantees. While this is a first cut, possible extensions include imposing a metric structure among the latent classes. We hope that this framework influences and guides practical implementations in the future.
