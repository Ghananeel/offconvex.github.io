---
layout:     post
title:      Exponential Learning Rate Schedules for Deep Learning (Part 2)
date:       2020-05-05 10:00:00
author:     Zhiyuan Li 
visible:    False
---

This blog post continues the previous post [Exponential Learning Rate Schedules for Deep Learning (Part 1)](http://www.offconvex.org/2020/04/24/ExpLR1/).

In the previous post, we presented our recent surprising discovery about LR that every net produced by Weight Decay + Constant LR + BN + Momentum can also be produced (in function space) via ExpLR + BN + Momentum and claimed it's provable. In this blog post, we will sketch the proof as promised. In detail, our goal is to prove the following main theorem when momentum is turned off. 

> **Theorem 1(Main, Informal).** SGD on a scale-invariant objective with initial learning rate $\eta$, weight decay factor $\lambda$, and momentum factor $\gamma$ is equivalent to SGD with momentum factor $\gamma$ where at iteration $t$, the ExpLR $\tilde{\eta}_ t$  is defined as $\tilde{\eta}_ t = \alpha^{-2t-1} \eta$ without weight decay($\tilde{\lambda} = 0$) where $\alpha$ is a non-zero root of equation 
     $$ x^2-(1+\gamma-\lambda\eta)x + \gamma = 0, $$
     
>    Specifically, when momentum $\gamma=0$,  the above schedule can be simplified as $\tilde{\eta}_ t = (1-\lambda\eta)^{-2t-1} \eta$.

## Proof Sketch for the Momentum-Free Case ($\gamma$ = 0)

The high-level idea is to use **scale invariance** brought by BN or other normalization schemes to build up a *per-step equivalence* between the trajectories of GD with weight decay and GD with ExpLR in function space.

Recall that loss $L$ is scale-invariant means $ L (c\cdot \pmb{\theta}) = L(\pmb{\theta}), \  \forall \pmb{\theta}, \forall c >0.$
For batch ${\mathcal{B}} = \\{ x_ i \\} _ {i=1}^B$, network parameter ${\pmb{\theta}}$, we  denote the network by $f_ {\pmb{\theta}}$ and the loss function at iteration $t$ by $L_ t(f_ {\pmb{\theta}}) = L(f_ {\pmb{\theta}}, {\mathcal{B}}_ t)$ . We also use $L_ t({\pmb{\theta}})$ for convenience. We say the network $f_ {\pmb{\theta}}$ is *scale invariant* if $\forall c>0$, $f_ {c{\pmb{\theta}}} = f_ {\pmb{\theta}}$, which implies the loss $L_ t$ is also scale invariant, i.e., $L_  t(c{\pmb{\theta}}_ t)=L_ t({\pmb{\theta}}_ t)$, $\forall c>0$. A key source of intuition is the following lemma provable via chain rule:

>**Lemma 1**. A scale-invariant loss $L$ satisfies
>(1). $\langle\nabla_ {\pmb{\theta}} L, {\pmb{\theta}} \rangle=0$ ;  
>(2). $\left.\nabla_ {\pmb{\theta}} L \right|_ {\pmb{\theta} = \pmb{\theta}_ 0} = c \left.\nabla_ {\pmb{\theta}} L\right|_  {\pmb{\theta} = c\pmb{\theta}_ 0}$, for any $c>0$.

<div style="text-align:center;">
<img style="width:400px;" src="http://www.cs.princeton.edu/~zl4/exp_lr_blog_images/inv_lemma.png" />
<br>
<b>Figure 1.</b> Illustration of Lemma 1. 
</div>
<br />
As shown in Figure 1, two equivalent networks ${\pmb{\theta}}$ and $c{\pmb{\theta}}$ are not equivalent anymore after one step of GD update. To keep them on the same track, the easist way is to scale up the learning rates from $\eta$ to $c^2\eta$. This observation suggests the following formalism, which views the tuple,  $({\pmb{\theta}},\eta)$, as the *state* of a training algorithm and thinks of each step in training as  a *mapping* from one state to another. Since mappings can be composed, any finite number of steps also correspond to a mapping.

1. Run GD with WD for a step:   &emsp;&emsp;&emsp;&emsp;&emsp; $\textrm{GD}^\rho_ t({\pmb{\theta}},\eta) =(\rho{\pmb{\theta}} - \eta \nabla L_ t({\pmb{\theta}}),\eta)$;  
   Run GD without WD for a step: &emsp;&emsp;&emsp;&emsp; $\textrm{GD}_ t:=\textrm{GD}^1_ t$;
2. Scale the parameter ${\pmb{\theta}}$:  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\Pi_ 1^c({\pmb{\theta}},\eta) = (c{\pmb{\theta}}, \eta)$;
3. Scale the LR $\eta$:  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\Pi_ 2^c({\pmb{\theta}},\eta) = ({\pmb{\theta}},c \eta)$.



<div style="text-align:center;">
<img style="width:700px;" src="http://www.cs.princeton.edu/~zl4/exp_lr_blog_images/equivalent_scaling.png" />
<br>
<b>Figure 2.</b> Proof of Lemma 2 between equivalent scaling and GD update. 
</div>
<br />

> **Lemma 2.** $\textrm{GD}_ t \circ \left[\Pi_ 2^{c^2}\circ\Pi_ 1^{c} \right] = \left[\Pi_ 2^{c^2}\circ\Pi_ 1^{c} \right] \circ \textrm{GD}_ t$, $\forall c>0$. We call $\Pi_ 2^{c^2}\circ\Pi_ 1^{c}$ an equivalent scaling.

The above lemma says that two maps differed by an equivalent scaling are equivalent if all the following mappings are GD updates and scalings. Reason being that since equivalent scaling commutes with all future mappings, we can always postpone it to the end of the mapping list. With the above notations, we restate the Theorem 1 ($\gamma=0$) below, where $\rho$ is picked as $1-\lambda\eta$.


<img style="width:900px;" src="http://www.cs.princeton.edu/~zl4/exp_lr_blog_images/Main_Thm_white.png" />


**Proof of Theorem 1 ($\gamma=0$):**  It's easy to check that $\textrm{GD}^ \rho_ t =\Pi_ 2^{\rho}\circ\Pi_ 1^{\rho} \circ \textrm{GD}_ t\circ \Pi_ 2^{\rho^{-1}} $ $= \left[\Pi_ 2^{\rho^2}\circ\Pi_ 1^{\rho} \right]$ $\circ \Pi_ 2^{\rho^{-1}}\circ\textrm{GD}_ t\circ \Pi_ 2^{\rho^{-1}}$. Thus $\textrm{GD}^\rho_ t$ and $\Pi_ 2^{\rho^{-1}}\circ \textrm{GD}_ t \circ \Pi_ 2^{\rho^{-1}}$ are only up to an equivalent scaling, which immediately implies the special case where momentum $\gamma=0$.


<div style="text-align:center;">
<img style="width:850px;" src="http://www.cs.princeton.edu/~zl4/exp_lr_blog_images/proof_sketch.png" />
<br>
<b>Figure 3.</b> Proof of Thm 1. Since Lemma 2 shows that all maps commute, the pink path and the black path represents the same map. We can further verify that the black path represents the left-hand side (GD with WD) of the restated Thm 1, and the pink path represents (GD with ExpLR) the right-hand side of the restated Thm 1.
</div>
<br />

## Practical Training Can't Be Captured by Gradient Flow

*Canonical framework for analyzing 1st order methods* focuses on proving that each ---or most---steps of GD noticeably reduce the objective, by relying on some assumption about the spectrum norm of the hessian of the loss, and most frequently, the *smoothness*, denoted by $\beta$.
Specifically, for GD update ${\pmb{\theta}}_ {t+1} = {\pmb{\theta}}_ {t} - \eta\nabla L({\pmb{\theta}}_ t)$, we have 
$L({\pmb{\theta}}_ {t+1}) - L({\pmb{\theta}}_ t) \le  ({\pmb{\theta}}_ {t+1}-{\pmb{\theta}}_ t)^\top \nabla L({\pmb{\theta}}_ t) $ $+ \frac{\beta}{2} \|{\pmb{\theta}}_ {t+1}-{\pmb{\theta}}_ t\|_ 2^2 = -\eta(1-\frac{\beta\eta}{2})\|\nabla L({\pmb{\theta}}_ t)\|_ 2^2.$
When $\eta<\frac{2}{\beta}$, the first order term is larger than the second order one, guaranteeing the loss value decreases. 


The *Canonical Framework* can be thought of as a discretization of continuous gradient descent (i.e., gradient flow). In principle, it is possible to use an arbitrarily small learning rate, but one uses a finite learning rate merely to keep the number of iterations small. The discrete process approximates the continuous process for $\tau = t\eta$ due to smoothness being small. 

$$\frac{d{\pmb{\theta}}_ \tau}{d\tau} = -\nabla L({\pmb{\theta}}_ \tau) \Longrightarrow \frac{dL({\pmb{\theta}}_ \tau)}{d\tau} = -\|\nabla L({\pmb{\theta}}_ \tau)\|_ 2^2.$$

However, the gradient flow of ExpLR shares the same trajectory as that of constant LR schedule --- they just differ by an exponential rescaling in LR (or in time), which doesn't affect the destination. Thus in the case of ExpLR, the discrete process is not well approximated by the continuous process because there is a big experimental difference between SGD with ExpLR and constant LR schedule.  This also suggests that any explanation of the benefits of ExpLR (or weight decay) may need to rely on the discrete process being somehow better. 


## Suggestion: a new parameterization for Weight Decay

One difficulty towards systematically understanding the behavior of the SGD is that the impacts of different hyperparameters are highly entangled. Even when fixing the rest ones and tuning the only hyperparameter,  the changes or trends in performance might not be monotone and could be complicated. We believe the following suggestion could reduce the entanglement between LR and WD.

**Suggestion for training nets with BN and (momentum) SGD**: Use this parametrization: ${\pmb{\theta}}_ {t+1} =(1-\lambda_ {new}){\pmb{\theta}}_ t - \eta \nabla L_ t({\pmb{\theta}})$, instead of the standard one, ${\pmb{\theta}}_ {t+1} =(1-\lambda_ {old}\eta){\pmb{\theta}}_ t - \eta \nabla L_ t({\pmb{\theta}})$.
And in order to mimic Step Decay, addition to dividing the LR by some constant, for the new update rule, we also need to divide the WD factor by the same constant. Interestingly, [our paper](https://arxiv.org/pdf/1910.07454.pdf) experimentally shows that shrinking the WD factor is optional and SGD might achieve similar test accruacy eventually    without it.

From the viewpoint of ExpLR, it's clear that LR $\eta$ has two roles in the standard parametrization, the initial LR ($\eta_ 0 = \eta$) and the exponent of LR growing (i.e. $(1-\lambda_ {old}\eta)^ {-2}$ for the momentum-free case). As a result, changing either $\eta$ or $\lambda_ {old}$ would change the exponent $\eta\lambda _ {old}$, to which the training is very sensitive.  By using the new update rule and picking the new WD factor $\lambda_ {new}$ as $\lambda_ {old}\eta$, training is not that sensitive to LR $\eta$. This is because one can prove that for the new parametrization,  the impact on the training dynamics of LR, $\eta$ and the scale of initialization, $\|\pmb{\theta}_ 0\|_ 2$ can be characterized by a single new hyperparameter $\frac{\eta}{\|{\pmb{\theta}}_ 0\|_ 2^2}$, and people already empirically observe the training of deep nets is not sensitive to the scale of initialization! 


## Concluding Thoughts

So far, we've built the connection between ExpLR and weight decay for modern deep nets. However, we are still far from understanding the generalization of ExpLR, because we don't even know how sgd + weight decay generalizes! This connection also forces us to rethink the role of weight decay in deep learning. Putting a norm-based regularization on an object independent of the norm, doesn't it sound silly? Maybe there're other similar tricks in deep learning, which works perfectly but just not in the way it is explained, and they definitely worth further theoretical investigation.

Another intriguing phenomenon that lacks understanding is the sudden drop of both train/test error when decaying LR. We have some interesting experimental observations about it in the paper. Still, to fully understand the trajectory, it seems to require some new mathematics because, as we mentioned early, the trajectory is not entirely in the smooth region, and continuous methods don't give good approximations.
